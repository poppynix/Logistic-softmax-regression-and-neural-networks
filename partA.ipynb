{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2191d47b",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "\n",
    "torch – the main PyTorch library for building and training neural networks.\n",
    "\n",
    "numpy – used for numerical operations and array manipulations.\n",
    "\n",
    "torchvision.datasets and torchvision.transforms – for loading and preprocessing image datasets.\n",
    "\n",
    "TensorDataset and DataLoader from torch.utils.data – to wrap data into tensors (multi-dimensional arrays) and efficiently load it in batches during training.\n",
    "\n",
    "train_test_split from sklearn.model_selection – to split our dataset into training and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac1988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7d951",
   "metadata": {},
   "source": [
    "## Defining Data Transformations\n",
    "\n",
    "#### transforms.ToTensor()\n",
    "Converts the image (originally a PIL image or NumPy array) into a PyTorch tensor and automatically scales pixel values from [0, 255] → [0, 1].\n",
    "\n",
    "#### transforms.Lambda(lambda x: x.flatten())\n",
    "Flattens each image tensor from shape (1, 28, 28) (grayscale 28×28 pixels) into a 1D vector of 784 values.\n",
    "\n",
    "This is useful for models that expect input as a single vector rather than a 2D image (like a simple linear model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d6f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.flatten())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ccdb1",
   "metadata": {},
   "source": [
    "## Loading and Preparing the MNIST Dataset\n",
    "\n",
    "In this step, we load the MNIST handwritten digits dataset and prepare it for training.\n",
    "\n",
    "#### datasets.MNIST\n",
    "Loads the MNIST dataset directly from torchvision.\n",
    "\n",
    "    train=True → loads the training split (60,000 images)\n",
    "\n",
    "    train=False → loads the test split (10,000 images)\n",
    "\n",
    "    transform=linear_model_transforms → applies the preprocessing we defined earlier (convert to tensor + flatten)\n",
    "\n",
    "#### Combining both splits\n",
    "The code merges the training and test datasets into one full dataset to later perform a custom stratified split.\n",
    "\n",
    "X_combined = torch.cat([...], dim=0).numpy()\n",
    "\n",
    "y_combined = torch.cat([...], dim=0).numpy()\n",
    "\n",
    "\n",
    "    torch.cat([...], dim=0) → concatenates tensors along the first dimension (stacking all samples together).\n",
    "\n",
    "    .data.float().div(255).flatten(start_dim=1) → converts images to floating-point numbers, scales pixel values from 0–255 to 0–1, and flattens them from (1, 28, 28) to (784,).\n",
    "\n",
    "    .numpy() → converts PyTorch tensors to NumPy arrays for later use with scikit-learn’s train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca3f4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "train_set_full = datasets.MNIST('./data', train=True, download=True, transform=linear_model_transforms)\n",
    "test_set_full = datasets.MNIST('./data', train=False, download=True, transform=linear_model_transforms)\n",
    "\n",
    "X_combined = torch.cat([train_set_full.data.float().div(255).flatten(start_dim=1),\n",
    "                        test_set_full.data.float().div(255).flatten(start_dim=1)], dim=0).numpy()\n",
    "y_combined = torch.cat([train_set_full.targets, test_set_full.targets], dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672db572",
   "metadata": {},
   "source": [
    "## Stratified Train–Validation–Test Split\n",
    "\n",
    "Now that we have the full MNIST dataset, we divide it into training, validation, and test sets — while keeping the class distribution balanced across all subsets.\n",
    "\n",
    "#### Split Proportions\n",
    "\n",
    "    Training set: 60% (≈ 42,000 samples)\n",
    "\n",
    "    Validation set: 20% (≈ 14,000 samples)\n",
    "\n",
    "    Test set: 20% (≈ 14,000 samples)\n",
    "\n",
    "To achieve this, we perform the split in two steps using scikit-learn’s train_test_split with the stratify argument (ensuring equal digit distribution in all sets).\n",
    "\n",
    "#### Why Stratify is Important\n",
    "\n",
    "Imagine you’re working with the MNIST dataset — it has 10 digit classes (0–9), each with about the same number of samples.\n",
    "If you split your dataset randomly without stratification, you could end up with something like:\n",
    "\n",
    "Training set: more 0’s and 1’s, fewer 8’s and 9’s\n",
    "\n",
    "Test set: missing some digits entirely \n",
    "\n",
    "That would make your model biased and your evaluation unreliable.\n",
    "\n",
    "By setting stratify=y, the function ensures each class appears in the same proportion in all subsets (train, validation, and test).\n",
    "\n",
    "#### Convert Back to Tensors\n",
    "\n",
    "After splitting, all NumPy arrays are converted back to PyTorch tensors using:\n",
    "\n",
    "    X_train, X_val, X_test = map(torch.tensor, (X_train, X_val, X_test))\n",
    "    y_train, y_val, y_test = map(torch.tensor, (y_train, y_val, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19c17d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 42000 (60%)\n",
      "Validation set size: 14000 (20%)\n",
      "Test set size: 14000 (20%)\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "TEST_VALID_SIZE = 0.4 \n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_combined, y_combined, \n",
    "    test_size=TEST_VALID_SIZE, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=y_combined \n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=y_temp \n",
    ")\n",
    "\n",
    "X_train, X_val, X_test = map(torch.tensor, (X_train, X_val, X_test))\n",
    "y_train, y_val, y_test = map(torch.tensor, (y_train, y_val, y_test))\n",
    "\n",
    "print(f\"Train set size: {len(X_train)} ({len(X_train)/70000*100:.0f}%)\") \n",
    "print(f\"Validation set size: {len(X_val)} ({len(X_val)/70000*100:.0f}%)\") \n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/70000*100:.0f}%)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d98ef5",
   "metadata": {},
   "source": [
    "## Creating DataLoaders\n",
    "\n",
    "Once the dataset is split into training, validation, and test sets, we wrap them in PyTorch DataLoaders.\n",
    "These objects handle batching, shuffling, and efficient iteration during training — making model training smoother and faster.\n",
    "\n",
    "#### Batch Size\n",
    "Each batch contains 64 samples.\n",
    "This value is a common default — it balances training speed and memory usage, but it can be tuned later for performance optimization.\n",
    "\n",
    "#### Creating TensorDatasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "Each TensorDataset pairs input tensors (X_*) with their corresponding labels (y_*), forming mini-datasets that PyTorch can iterate over.\n",
    "\n",
    "#### Creating DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "shuffle=True for the training set → ensures the model sees data in a different order each epoch (helps generalization).\n",
    "\n",
    "shuffle=False for validation/test → keeps order consistent for evaluation.\n",
    "\n",
    "#### Optional Dictionary Storage\n",
    "    DATA_LOADERS = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader,\n",
    "        'test': test_loader\n",
    "    }\n",
    "\n",
    "A convenient way to store all loaders together for easier access across multiple notebooks or assignment parts (e.g., A2, A3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 \n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "DATA_LOADERS = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader,\n",
    "    'test': test_loader\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
